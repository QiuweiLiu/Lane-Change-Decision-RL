import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


class PPOActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_size=128, use_attention=False, attn_dims=None):
        super(PPOActorCritic, self).__init__()
        self.use_attention = use_attention
        self.main_dim = attn_dims["main_dim"]
        self.veh_dim = attn_dims["veh_dim"]
        self.veh_count = attn_dims["veh_count"]

        # å¦‚æœå¯ç”¨ attention æœºåˆ¶
        if use_attention:
            # ä¸»è½¦å’Œè½¦è¾†çš„åµŒå…¥å±‚
            self.main_embed = nn.Linear(self.main_dim, hidden_size)
            self.veh_embed = nn.Linear(self.veh_dim, hidden_size)
            # è®¡ç®—æœ€ç»ˆè¾“å…¥ç‰¹å¾å¤§å°
            self.fc_final = nn.Linear(hidden_size * 2 + 3, hidden_size)  # æ‹¼æ¥ï¼šä¸»è½¦ + æ¦‚ç‡ + å¯¹æŠ—è½¦è¾†
        else:
            self.shared_fc1 = nn.Linear(state_dim, hidden_size)
            self.shared_fc2 = nn.Linear(hidden_size, hidden_size)

        # ç­–ç•¥å¤´å’Œä»·å€¼å¤´
        self.policy_head = nn.Linear(hidden_size, action_dim)
        self.value_head = nn.Linear(hidden_size, 1)

    def forward(self, x, action=None, prob=None):
        """
        :param x: çŠ¶æ€è¾“å…¥ [B, main_dim + veh_count * veh_dim]
        :param action: å½“å‰é€‰æ‹©çš„åŠ¨ä½œï¼ˆç”¨äºè®¡ç®— attention æœºåˆ¶ï¼‰
        :param prob: ç½‘ç»œè¾“å‡ºçš„æ¢é“æ¦‚ç‡ [B, 3]ï¼ˆç”¨äºè®¡ç®— Attn-1ï¼‰
        """
        if self.use_attention:
            B = x.shape[0]
            main_feat = x[:, :self.main_dim]  # [B, main_dim]
            veh_feat = x[:, self.main_dim:].view(B, self.veh_count, self.veh_dim)  # [B, veh_count, veh_dim]

            # ä¸»è½¦åµŒå…¥
            main_embed = self.main_embed(main_feat)  # [B, H]

            # Attn-1: åŸºäºåŠ¨ä½œä¸æ¦‚ç‡ä¹‹é—´çš„åç¦»è¿›è¡ŒåŠ æƒ
            if action is not None and prob is not None:
                action_onehot = F.one_hot(action, num_classes=3).float()  # [B, 3]
                inverse = 1.0 - action_onehot  # å–ååŠ¨ä½œ
                diff = (inverse - prob).abs().sum(dim=1, keepdim=True)  # [B, 1] åç¦»è®¡ç®—
                gate = torch.sigmoid(2.0 * diff)  # åŠ æƒå› å­
                weighted_prob = gate * prob  # [B, 3]
            else:
                weighted_prob = torch.zeros(B, 3).to(x.device)  # é»˜è®¤æ¦‚ç‡ä¸º0

            # Attn-2: åŸºäºä¸æ ‡å‡†å±é™©è½¦è¾†çš„ç›¸ä¼¼åº¦è¿›è¡ŒåŠ æƒ
            danger_std = torch.tensor([-5.0, 0.0, -5.0, 0.0, 0, 0, 0], dtype=torch.float32).to(x.device)
            dist = torch.norm(veh_feat - danger_std, dim=-1)  # [B, veh_count]
            weights = F.softmax(-dist, dim=1)  # è·ç¦»è¶Šè¿‘ï¼Œæƒé‡è¶Šå¤§
            veh_embeds = self.veh_embed(veh_feat)  # [B, veh_count, H]
            weighted_veh = torch.sum(weights.unsqueeze(-1) * veh_embeds, dim=1)  # [B, H]

            # æ‹¼æ¥ï¼šä¸»è½¦åµŒå…¥ + æ¢é“æ¦‚ç‡ + å¯¹æŠ—è½¦è¾†åŠ æƒ
            final_input = torch.cat([main_embed, weighted_prob, weighted_veh], dim=1)  # [B, 2H + 3]
            x = F.relu(self.fc_final(final_input))  # [B, H]
        else:
            x = F.relu(self.shared_fc1(x))  # [B, H]
            x = F.relu(self.shared_fc2(x))  # [B, H]

        return self.policy_head(x), self.value_head(x)


class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.98, eps_clip=0.2,
                 entropy_coef=0.01, use_attention=False, attn_dims=None):
        self.use_attention = use_attention
        self.model = PPOActorCritic(state_dim, action_dim, use_attention=use_attention, attn_dims=attn_dims)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.entropy_coef = entropy_coef
        self.best_reward_state = None  # ç”¨äºå­˜å‚¨å†å²æœ€ä½³çŠ¶æ€
        self.best_reward = -np.inf  # åˆå§‹åŒ–æœ€å¥½çš„å¥–åŠ±ä¸ºè´Ÿæ— ç©·

    def update_best_reward_state(self, episode_reward, state):
        """æ›´æ–°æœ€å¥½çš„å¥–åŠ±çŠ¶æ€"""
        if episode_reward > self.best_reward:
            self.best_reward = episode_reward
            self.best_reward_state = state

    def select_action(self, state, prob=None):
        """æ ¹æ®çŠ¶æ€é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        if self.use_attention and prob is not None:
            logits, _ = self.model(state, action=None, prob=prob)
        else:
            logits, _ = self.model(state)
        probs = torch.softmax(logits, dim=-1)
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action), dist.entropy()

    def evaluate(self, states, actions, probs=None):
        """è¯„ä¼°å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ä¸ä»·å€¼"""
        if self.use_attention and probs is not None:
            logits, values = self.model(states, action=actions, prob=probs)
        else:
            logits, values = self.model(states)
        probs_dist = torch.softmax(logits, dim=-1)
        dist = torch.distributions.Categorical(probs_dist)
        return dist.log_prob(actions), torch.squeeze(values), dist.entropy()

    def update(self, memory, batch_size=64, epochs=4):
        """æ›´æ–°ç­–ç•¥ç½‘ç»œ"""
        states = torch.FloatTensor(np.array(memory['states']))
        actions = torch.LongTensor(np.array(memory['actions']))
        old_logprobs = torch.FloatTensor(np.array(memory['logprobs']))
        returns = torch.FloatTensor(np.array(memory['returns']))
        advantages = returns - self.model(states)[1].squeeze().detach()
        probs = torch.FloatTensor(np.array(memory['probs'])) if self.use_attention else None

        for _ in range(epochs):
            for i in range(0, len(states), batch_size):
                idx = slice(i, i + batch_size)
                s_batch, a_batch = states[idx], actions[idx]
                lp_old, adv, ret = old_logprobs[idx], advantages[idx], returns[idx]
                p_batch = probs[idx] if self.use_attention else None

                logprobs, values, entropy = self.evaluate(s_batch, a_batch, p_batch)
                ratios = torch.exp(logprobs - lp_old)
                surr1, surr2 = ratios * adv, torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * adv
                loss = -torch.min(surr1, surr2).mean() + 0.5 * F.mse_loss(values,
                                                                          ret) - self.entropy_coef * entropy.mean()

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()



    def save(self, path):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load(self, path):
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.model.eval()


def compute_gae(rewards, masks, values, gamma=0.98, lam=0.90):
    """
    è®¡ç®—Generalized Advantage Estimation (GAE)

    :param rewards: æ¯ä¸€æ­¥çš„å¥–åŠ±
    :param masks: æ¯ä¸€æ­¥æ˜¯å¦ç»“æŸçš„æ ‡è®°ï¼ˆdoneï¼‰ï¼Œ0è¡¨ç¤ºç»“æŸï¼Œ1è¡¨ç¤ºæœªç»“æŸ
    :param values: æ¯ä¸€æ­¥çš„çŠ¶æ€ä»·å€¼
    :param gamma: æŠ˜æ‰£å› å­ï¼Œé»˜è®¤ä¸º0.99
    :param lam: GAEçš„lambdaå‚æ•°ï¼Œé»˜è®¤ä¸º0.95
    :return: æ¯ä¸€æ­¥çš„ä¼˜åŠ¿å€¼
    """
    values = values + [0]  # å°†valuesçš„æœ«å°¾æ·»åŠ ä¸€ä¸ª0ï¼ˆå¤„ç†æœ€åä¸€æ­¥ï¼‰
    gae = 0
    returns = []

    # ä»æœ€åä¸€æ­¥å¾€å‰è®¡ç®—GAE
    for step in reversed(range(len(rewards))):
        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]  # è®¡ç®—delta
        gae = delta + gamma * lam * masks[step] * gae  # æ›´æ–°GAE
        returns.insert(0, gae + values[step])  # æ’å…¥è¿”å›å€¼ï¼Œä¿ç•™é¡ºåº

    return returns


def apply_her(memory, trajectory, env, alpha=0.3):
    """ä½¿ç”¨HERæœºåˆ¶çš„å¥–åŠ±è°ƒæ•´"""
    if env.best_reward_state is None:
        return memory

    goal_state = env.best_reward_state[:2]  # ç›®æ ‡ä½ç½®

    new_rewards = []
    for (state, action, old_reward, next_state, done) in trajectory:
        distance = np.linalg.norm(state[:2] - goal_state)
        her_reward = 1.0 if distance < 5.0 else 0.0
        fused_reward = old_reward + alpha * her_reward
        new_rewards.append(fused_reward)

    memory['rewards'] = new_rewards
    return memory


def train_ppo(env, agent, max_episodes=300, max_steps=5000, use_her=False, use_dlc_input=False):
    reward_curve = []
    collision_count = 0
    total_speed_all = 0.0
    speed_count_all = 0.0
    lane_attempt_count = 0
    lane_success_count = 0

    for episode in range(max_episodes):
        decay_lr =  max(2e-4, 5e-4 * (1 - episode / max_episodes))
        agent.optimizer.param_groups[0]['lr'] = decay_lr
        agent.entropy_coef = max(0.005, 0.02 * (1 - episode / max_episodes))  # æ›´é«˜åˆå§‹ç†µæƒé‡
        agent.eps_clip = 0.2

        state = env.reset()
        memory = {
            'states': [], 'actions': [], 'logprobs': [],
            'rewards': [], 'masks': [], 'values': [], 'returns': [],
            'probs': []
        }

        episode_reward = 0
        trajectory = []
        attempted_lane_change = False
        lane_id_before = None

        for step in range(max_steps):
            if step % 100 == 0:
                attempted_lane_change = False
                lane_id_before = env.world.get_map().get_waypoint(env.cars['maincar'].get_location()).lane_id

            prob = env._get_lane_change_probs() if use_dlc_input else np.array([0.3, 0.4, 0.3], dtype=np.float32)
            action, logprob, entropy = agent.select_action(state, prob=prob)
            next_state, reward, done, _ = env.step(action)

            with torch.no_grad():
                value = agent.model(torch.FloatTensor(state).unsqueeze(0),
                                    action=torch.tensor([action]),
                                    prob=torch.tensor(prob).unsqueeze(0))[1]

            memory['states'].append(state)
            memory['actions'].append(action)
            memory['logprobs'].append(logprob.item())
            memory['rewards'].append(reward)
            memory['masks'].append(1 - done)
            memory['values'].append(value.item())
            memory['probs'].append(prob)

            if use_her:
                trajectory.append((state, action, reward, next_state, done))

            if action in [1, 2]:
                attempted_lane_change = True

            if (step + 1) % 100 == 0 and attempted_lane_change:
                lane_id_after = env.world.get_map().get_waypoint(env.cars['maincar'].get_location()).lane_id
                lane_attempt_count += 1
                if lane_id_after != lane_id_before:
                    lane_success_count += 1

            speed = np.linalg.norm([env.cars['maincar'].get_velocity().x,
                                    env.cars['maincar'].get_velocity().y])
            total_speed_all += speed
            speed_count_all += 1

            state = next_state
            episode_reward += reward
            if done:
                break

        if env.collision_event:
            collision_count += 1

        if use_her:
            memory = apply_her(memory, trajectory, env, alpha=0.3)

        memory['returns'] = compute_gae(memory['rewards'], memory['masks'], memory['values'])
        agent.update(memory)
        reward_curve.append(episode_reward)
        print(f"Episode {episode + 1}: Total Reward = {episode_reward:.2f}")

    print(f"Total Collisions: {collision_count}")
    print(f"Collision Rate: {collision_count / max_episodes:.2%}")
    if lane_attempt_count > 0:
        print(f"Lane Change Success Rate: {lane_success_count / lane_attempt_count:.2%}")
    else:
        print("No lane change attempts")
    print(f"Average Speed: {total_speed_all / speed_count_all:.2f} m/s")

    # Plot reward curve
    plt.plot(reward_curve)
    plt.title("Reward Convergence Curve")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.grid()
    plt.savefig("reward_curve.png")
    plt.close()

    # ==== â¬ä¿å­˜ç»Ÿè®¡ç»“æœã€å¥–åŠ±CSVã€çƒ­å›¾ï¼ˆå¯é€‰ï¼‰ ====

    # 1. ä¿å­˜ summary.txt
    # åœ¨ f-string ä¹‹å‰å…ˆè®¡ç®—æ¢é“æˆåŠŸç‡å­—ç¬¦ä¸²
    if lane_attempt_count > 0:
        lane_success_rate_str = f"{lane_success_count / lane_attempt_count:.2%}"
    else:
        lane_success_rate_str = "0.00%"

    summary = f"""ğŸš— è®­ç»ƒæ€»ç»“ï¼ˆPPOæ¨¡å‹ï¼‰
    -------------------------
    æ€»å›åˆæ•°         : {max_episodes}
    æ€»ç¢°æ’æ¬¡æ•°       : {collision_count}
    ç¢°æ’ç‡           : {collision_count / max_episodes:.2%}

    æ¢é“å°è¯•æ¬¡æ•°     : {lane_attempt_count}
    æ¢é“æˆåŠŸæ¬¡æ•°     : {lane_success_count}
    æ¢é“æˆåŠŸç‡       : {lane_success_rate_str}

    å¹³å‡é€Ÿåº¦         : {total_speed_all / speed_count_all:.2f} m/s
    å¹³å‡æ¯å›åˆå¥–åŠ±   : {np.mean(reward_curve):.2f}"""

    with open("training_summary.txt", "w", encoding="utf-8") as f:
        f.write(summary.strip())

    # 2. ä¿å­˜æ¯å›åˆå¥–åŠ±ä¸º CSV
    reward_df = pd.DataFrame({
        "episode": list(range(1, len(reward_curve) + 1)),
        "reward": reward_curve
    })
    reward_df.to_csv("episode_rewards.csv", index=False)

    # 3. è‹¥å¯ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ™ä¿å­˜æ³¨æ„åŠ›çƒ­å›¾ï¼ˆæ¨¡æ‹Ÿå€¼ï¼‰
    if agent.use_attention:
        attn_matrix = np.random.rand(env.attn_dims["veh_count"], env.attn_dims["veh_dim"])
        plt.figure(figsize=(10, 4))
        sns.heatmap(attn_matrix, cmap="YlOrBr", annot=True)
        plt.title("Attention Heatmap (ç¤ºæ„)")
        plt.xlabel("Feature Index")
        plt.ylabel("Opponent Vehicle")
        plt.tight_layout()
        plt.savefig("attention_heatmap.png")
        plt.close()

    return agent
