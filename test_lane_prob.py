# ä» lane_prob_module å¯¼å…¥ get_lane_prob å’Œ TrajectoryBuffer
from lane_prob_module import get_lane_prob, TrajectoryBuffer
from env_setup import ENV
import logging
import numpy as np






def test_dlc_policy(episodes=5, max_steps=500, model_path="ppo_model.pth"):
    all_rewards = []
    total_lane_attempts = 0
    total_lane_successes = 0

    # åˆå§‹åŒ–ç¯å¢ƒ
    config = {'host': 'localhost', 'port': 2000}
    logger = logging.getLogger("TEST")
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        ch = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    # åˆ›å»ºç¯å¢ƒå¯¹è±¡
    env = ENV(config, logger, use_dlc_input=True)  # ç¯å¢ƒåˆå§‹åŒ–


    for ep in range(episodes):
        state = env.reset()
        ep_reward = 0
        lane_changed = False
        lane_id_before = None

        # è·å–ä¸»è½¦å’Œå‘¨å›´è½¦è¾†çš„å¯¹è±¡
        maincar = env.cars['maincar']
        surrounding_cars = [actor for actor in env.actor_list if actor != maincar]  # è·å–å‘¨å›´çš„è½¦è¾†

        # åˆ›å»º TrajectoryBuffer å®ä¾‹ï¼Œç”¨äºå­˜å‚¨è½¨è¿¹æ•°æ®
        buffer = TrajectoryBuffer(max_len=48)

        for step in range(max_steps):
            # è·å–ç¥ç»ç½‘ç»œè¾“å‡ºçš„æ¢é“æ¦‚ç‡
            lane_probs = get_lane_prob(maincar, surrounding_cars, buffer, model_path)  # ä½¿ç”¨ lane_prob_model ä¸­çš„ get_lane_prob

            # ==================== æ£€æŸ¥ 1: è¾“å…¥ç»´åº¦æ£€éªŒ ====================
            if state.shape[0] != env.observation_space.shape[0]:

                break  # æˆ–è€…è·³è¿‡å½“å‰æ­¥ï¼Œå…·ä½“å–å†³äºä½ æƒ³å¦‚ä½•å¤„ç†è¿™ä¸ªé”™è¯¯

            # ==================== æ¯100æ­¥æ‰“å°æ¢é“æ¦‚ç‡ ====================
            if step % 100 == 0:
                print(f"ğŸ” Step {step + 1} æ¢é“æ¦‚ç‡: {lane_probs}")

            # å†³ç­–é€»è¾‘ï¼šè‹¥å·¦æˆ–å³æ¦‚ç‡ > 0.5ï¼Œåˆ™æ¢é“
            if lane_probs[1] > 0.5:
                action = 1  # å·¦æ¢é“
            elif lane_probs[2] > 0.5:
                action = 2  # å³æ¢é“
            else:
                action = 0  # ä¿æŒ

            # è®°å½•æ¢é“å°è¯•
            if step % 100 == 0:
                lane_changed = False
                lane_id_before = env.world.get_map().get_waypoint(env.cars['maincar'].get_location()).lane_id

            _, reward, done, _ = env.step(action)
            ep_reward += reward

            # æ¢é“å°è¯•ç»Ÿè®¡
            if (step + 1) % 100 == 0 and action in [1, 2]:
                lane_id_after = env.world.get_map().get_waypoint(env.cars['maincar'].get_location()).lane_id
                total_lane_attempts += 1
                if lane_id_after != lane_id_before:
                    total_lane_successes += 1

            if done:
                break

        print(f"âœ… Episode {ep + 1}: æ€»å¥–åŠ± = {ep_reward:.2f}")
        all_rewards.append(ep_reward)

    avg_reward = np.mean(all_rewards)
    print(f"\nğŸ¯ å¹³å‡å¥–åŠ± = {avg_reward:.2f}")

    if total_lane_attempts > 0:
        success_rate = total_lane_successes / total_lane_attempts
        print(f"ğŸš— æ¢é“æˆåŠŸç‡: {success_rate:.2%} ({total_lane_successes}/{total_lane_attempts})")
    else:
        print("ğŸš— æ— æ¢é“å°è¯•")

    env.close()





if __name__ == "__main__":
    config = {'host': 'localhost', 'port': 2000}
    logger = logging.getLogger("DLC_TEST")
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        ch = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    # åˆå§‹åŒ–ç¯å¢ƒï¼Œå¯ç”¨ dlc_input ä»¥æ¿€æ´»ç¥ç»ç½‘ç»œ
    env = ENV(config, logger, use_dlc_input=True)

    # æ˜¾å¼ä¼ å…¥æ¢é“æ¦‚ç‡æ¨¡å‹è·¯å¾„
    test_dlc_policy(episodes=5, max_steps=2000, model_path="checkpoint.pth")



